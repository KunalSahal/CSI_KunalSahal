{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfadaa17",
   "metadata": {},
   "source": [
    "<b>Installing Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39944539",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install plotly\n",
    "%pip install scipy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5f14e",
   "metadata": {},
   "source": [
    "<b>Importing Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
    "\n",
    "''' Different ML algorithms '''\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "''' Hyperparameter Distributions '''\n",
    "from scipy.stats import uniform, randint\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd54da",
   "metadata": {},
   "source": [
    "<b>Loading the data and Exploring it</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4a6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: breast_cancer.csv\n",
      "Number of samples: 569\n",
      "Number of features: 30\n",
      "Target classes: ['malignant' 'benign']\n",
      "Class distribution: [212 357]\n"
     ]
    }
   ],
   "source": [
    "# Loading breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(f\"Dataset: {data.filename}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6594b",
   "metadata": {},
   "source": [
    "<b>Data preprocessing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2114b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (455, 30)\n",
      "Test set size: (114, 30)\n",
      "Features have been standardized using StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling (important for algorithms like SVM, KNN, Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(\"Features have been standardized using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec6cec",
   "metadata": {},
   "source": [
    "<b>Initialiing multiple ML models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e42f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models to compare: 7\n",
      "✓ Logistic Regression\n",
      "✓ Random Forest\n",
      "✓ Support Vector Machine\n",
      "✓ K-Nearest Neighbors\n",
      "✓ Gradient Boosting\n",
      "✓ Naive Bayes\n",
      "✓ Decision Tree\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store all models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Total models to compare: {len(models)}\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"✓ {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e1c3e",
   "metadata": {},
   "source": [
    "<b>Train and evaluate baseline models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e9de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy: 0.9825\n",
      "  F1-Score: 0.9825\n",
      "  Training Time: 0.0346s\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy: 0.9561\n",
      "  F1-Score: 0.9560\n",
      "  Training Time: 0.1953s\n",
      "\n",
      "Training Support Vector Machine...\n",
      "  Accuracy: 0.9825\n",
      "  F1-Score: 0.9825\n",
      "  Training Time: 0.0228s\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "  Accuracy: 0.9561\n",
      "  F1-Score: 0.9560\n",
      "  Training Time: 2.9685s\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy: 0.9561\n",
      "  F1-Score: 0.9558\n",
      "  Training Time: 0.6200s\n",
      "\n",
      "Training Naive Bayes...\n",
      "  Accuracy: 0.9386\n",
      "  F1-Score: 0.9384\n",
      "  Training Time: 0.0000s\n",
      "\n",
      "Training Decision Tree...\n",
      "  Accuracy: 0.9123\n",
      "  F1-Score: 0.9130\n",
      "  Training Time: 0.0147s\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "baseline_results = {}\n",
    "\n",
    "# Function to calculate comprehensive metrics\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Adding AUC for binary classification\n",
    "    if y_proba is not None:\n",
    "        metrics['AUC'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Training and evaluating each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training the model\n",
    "    if model_name in ['Support Vector Machine', 'K-Nearest Neighbors', 'Logistic Regression']:\n",
    "        # These models benefit from scaled features\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
    "    else:\n",
    "        # Tree-based models don't require scaling\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculating training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculating metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_proba)\n",
    "    metrics['Training Time'] = training_time\n",
    "    \n",
    "    # Storing results\n",
    "    baseline_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a16858",
   "metadata": {},
   "source": [
    "<b>Display baseline results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e57912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Performance:\n",
      "                        Accuracy  Precision  Recall  F1-Score     AUC  \\\n",
      "Logistic Regression       0.9825     0.9825  0.9825    0.9825  0.9954   \n",
      "Random Forest             0.9561     0.9561  0.9561    0.9560  0.9937   \n",
      "Support Vector Machine    0.9825     0.9825  0.9825    0.9825  0.9950   \n",
      "K-Nearest Neighbors       0.9561     0.9561  0.9561    0.9560  0.9788   \n",
      "Gradient Boosting         0.9561     0.9569  0.9561    0.9558  0.9907   \n",
      "Naive Bayes               0.9386     0.9384  0.9386    0.9384  0.9878   \n",
      "Decision Tree             0.9123     0.9161  0.9123    0.9130  0.9157   \n",
      "\n",
      "                        Training Time  \n",
      "Logistic Regression            0.0346  \n",
      "Random Forest                  0.1953  \n",
      "Support Vector Machine         0.0228  \n",
      "K-Nearest Neighbors            2.9685  \n",
      "Gradient Boosting              0.6200  \n",
      "Naive Bayes                    0.0000  \n",
      "Decision Tree                  0.0147  \n",
      "\n",
      "Best performing baseline model: Logistic Regression\n",
      "Best F1-Score: 0.9825\n"
     ]
    }
   ],
   "source": [
    "# Creating results DataFrame for better visualization\n",
    "results_df = pd.DataFrame(baseline_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(results_df)\n",
    "\n",
    "# Finding best performing model\n",
    "best_model_name = results_df['F1-Score'].idxmax()\n",
    "print(f\"\\nBest performing baseline model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff3f49",
   "metadata": {},
   "source": [
    "<b>Hyperparameter tuning with GridSearchCV</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba0681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing GridSearchCV for Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "# Defining parameter grids for top 3 models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'poly', 'linear']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Performing GridSearchCV for selected models\n",
    "grid_search_results = {}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for {model_name}...\")\n",
    "    \n",
    "    # Get the base model\n",
    "    base_model = models[model_name]\n",
    "    \n",
    "    # Creating GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,  # Use all available processors\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    start_time = time.time()\n",
    "    if model_name == 'Support Vector Machine':\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    tuning_time = time.time() - start_time\n",
    "    \n",
    "    # Get best model and evaluate\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Making predictions\n",
    "    if model_name == 'Support Vector Machine':\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_proba = best_model.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculating metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_proba)\n",
    "    metrics['Tuning Time'] = tuning_time\n",
    "    metrics['Best Params'] = grid_search.best_params_\n",
    "    metrics['CV Score'] = grid_search.best_score_\n",
    "    \n",
    "    grid_search_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"  Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"  Test F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Best Parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a16a7a",
   "metadata": {},
   "source": [
    "<b>Hyperparameter tuning with RandomizedSearchCV</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing RandomizedSearchCV for Random Forest...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Defining parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': uniform(0.1, 100),\n",
    "        'gamma': uniform(0.001, 1),\n",
    "        'kernel': ['rbf', 'poly', 'linear']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': randint(100, 300),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.6, 0.4)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Performing RandomizedSearchCV\n",
    "random_search_results = {}\n",
    "\n",
    "for model_name, param_dist in param_distributions.items():\n",
    "    print(f\"\\nPerforming RandomizedSearchCV for {model_name}...\")\n",
    "    \n",
    "    # Get the base model\n",
    "    base_model = models[model_name]\n",
    "    \n",
    "    # Creating RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of parameter settings sampled\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit RandomizedSearchCV\n",
    "    start_time = time.time()\n",
    "    if model_name == 'Support Vector Machine':\n",
    "        random_search.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        random_search.fit(X_train, y_train)\n",
    "    \n",
    "    tuning_time = time.time() - start_time\n",
    "    \n",
    "    # Get best model and evaluate\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Making predictions\n",
    "    if model_name == 'Support Vector Machine':\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_proba = best_model.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculating metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_proba)\n",
    "    metrics['Tuning Time'] = tuning_time\n",
    "    metrics['Best Params'] = random_search.best_params_\n",
    "    metrics['CV Score'] = random_search.best_score_\n",
    "    \n",
    "    random_search_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"  Best CV Score: {random_search.best_score_:.4f}\")\n",
    "    print(f\"  Test F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Best Parameters: {random_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
